

from __future__ import annotations

import math
import torch
from typing import TYPE_CHECKING

import omni.isaac.lab.utils.math as math_utils
from omni.isaac.lab.actuators import ActuatorNetMLP
from omni.isaac.lab.assets import Articulation
from omni.isaac.lab.envs.mdp import height_scan
from omni.isaac.lab.managers import SceneEntityCfg
from omni.isaac.lab.sensors import ContactSensor, RayCaster, RayCasterCamera
from nav_tasks.mdp import GoalCommand

from .actions import NavigationSE2Action

if TYPE_CHECKING:
    from omni.isaac.lab.envs import ManagerBasedRLEnv

"""
Root state.
"""


def base_orientation_xyzw(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Orientation of the asset's root in world frame.

    Note: converts the quaternion to (x, y, z, w) format."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    return asset.data.root_quat_w[:, [1, 2, 3, 0]]


def base_position(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Position of the asset's root in world frame."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    return asset.data.root_pos_w


def joint_torque(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Joint positions of the asset."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    return asset.data.applied_torque


def joint_pos(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Joint positions of the asset."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    return asset.data.joint_pos


def joint_vel(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Joint velocities of the asset."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    return asset.data.joint_vel


def joint_pos_error_history(
    env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"), history_idx: int = 0
) -> torch.Tensor:
    """Root linear velocity in the asset's root frame."""
    # extract the used quantities (to enable type-hinting)
    actuators: ActuatorNetMLP = env.scene[asset_cfg.name].actuators["legs"]
    return actuators._joint_pos_error_history[:, history_idx]


def joint_velocity_history(
    env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"), history_idx: int = 0
) -> torch.Tensor:
    """Root linear velocity in the asset's root frame."""
    # extract the used quantities (to enable type-hinting)
    actuators: ActuatorNetMLP = env.scene[asset_cfg.name].actuators["legs"]
    return actuators._joint_vel_history[:, history_idx]


def friction(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """The second to last low level action."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # get the friction coefficients
    return asset.root_physx_view.get_material_properties()[:, asset_cfg.body_ids, 0].to(env.device)


def se2_root_position(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """The root position of the asset in the SE(2) frame."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # get yaw angle of the root
    yaw = math_utils.euler_xyz_from_quat(asset.data.root_quat_w)[2]
    # return the root position in the SE(2)
    return torch.cat([asset.data.root_pos_w[:, :2], yaw.unsqueeze(-1)], dim=-1)


"""
Sensors
"""


def lidar2Dnormalized(env: ManagerBasedRLEnv, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
    """Lidar scan from the given sensor w.r.t. the sensor's frame."""
    # extract the used quantities (to enable type-hinting)
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    # return the height scan
    distances = torch.norm((sensor.data.ray_hits_w - sensor.data.pos_w[:, None, :]), dim=-1)
    # clip inf values to max_distance
    distances[torch.isinf(distances)] = sensor.cfg.max_distance
    # returned clipped to the sensor's range
    return torch.clip(distances, 0.0, sensor.cfg.max_distance) / sensor.cfg.max_distance


def raycast_depth_camera_data(env: ManagerBasedRLEnv, sensor_cfg: SceneEntityCfg, data_type: str) -> torch.Tensor:
    """Images generated by the raycast camera."""
    # extract the used quantities (to enable type-hinting)
    sensor: RayCasterCamera = env.scene.sensors[sensor_cfg.name]

    # return the data
    output = sensor.camera_data.output[data_type].clone().unsqueeze(-1)
    output[torch.isnan(output)] = sensor.cfg.max_distance
    output[torch.isinf(output)] = sensor.cfg.max_distance

    # normalize the data
    # output = torch.clip(output, 0.0, sensor.cfg.max_distance) / sensor.cfg.max_distance
    return output


def height_scan_clipped(
    env: ManagerBasedRLEnv, sensor_cfg: SceneEntityCfg, offset: float = 0.5, clip_height: float = 0.5
) -> torch.Tensor:
    """Height scan from the given sensor w.r.t. the sensor's frame.

    The provided offset (Defaults to 0.5) is subtracted from the returned values.
    """
    # extract the used quantities (to enable type-hinting)
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    # height scan: height = sensor_height - hit_point_z - 0.5
    height = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - offset
    # assign max distance to inf values
    height[torch.isinf(height)] = sensor.cfg.max_distance
    height[torch.isnan(height)] = sensor.cfg.max_distance

    # clip to max observable height
    height[height > clip_height] = clip_height

    return height


def height_scan_square(env: ManagerBasedRLEnv, sensor_cfg: SceneEntityCfg, shape: list[int] | None = None) -> torch.Tensor:
    """Height scan from the given sensor w.r.t. the sensor's frame given in the square pattern of the sensor."""
    # call regular height scanner function
    height = height_scan(env, sensor_cfg)
    shape = shape if shape is not None else [int(math.sqrt(height.shape[1])), int(math.sqrt(height.shape[1]))]
    # unflatten the height scan to make use of spatial information
    height_square = torch.unflatten(height, 1, (shape[0], shape[1]))
    # unqueeze to make compatible with convolutional layers
    return height_square.unsqueeze(1)


"""
Collision
"""


def base_collision(
    env: ManagerBasedRLEnv, threshold: float = 1.0, sensor_cfg: SceneEntityCfg = SceneEntityCfg("contact_forces")
) -> torch.Tensor:
    """Terminate when the contact force on the sensor exceeds the force threshold."""
    # extract the used quantities (to enable type-hinting)
    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
    net_contact_forces = contact_sensor.data.net_forces_w_history
    # check if any contact force exceeds the threshold
    return torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] > threshold


"""
Actions.
"""


def last_low_level_action(env: ManagerBasedRLEnv, action_term: str) -> torch.Tensor:
    """The last low-level action."""
    action_term: NavigationSE2Action = env.action_manager._terms[action_term]
    return action_term.low_level_actions


def second_last_low_level_action(env: ManagerBasedRLEnv, action_term: str) -> torch.Tensor:
    """The second to last low level action."""
    action_term: NavigationSE2Action = env.action_manager._terms[action_term]
    return action_term.prev_low_level_actions


"""
Commands.
"""


def vel_commands(env: ManagerBasedRLEnv, action_term: str) -> torch.Tensor:
    """The velocity command generated by the planner and given as input to the step function"""
    action_term: NavigationSE2Action = env.action_manager._terms[action_term]
    return action_term.processed_actions


def goal_command_w(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:
    """The generated command from command term in the command manager with the given name."""
    command_term: GoalCommand = env.command_manager._terms[command_name]
    return command_term.pos_command_w


"""
Energy consumption
"""


def energy_consumption(
    env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"), energy_scale_factor: float = 0.001
) -> torch.Tensor:
    """The energy consumption of the asset. Computed as the sum of the squared applied torques."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    return (asset.data.applied_torque**2).sum(dim=-1).unsqueeze(-1) * energy_scale_factor
