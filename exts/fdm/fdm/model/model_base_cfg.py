

from __future__ import annotations

from dataclasses import MISSING

from omni.isaac.lab.utils import configclass


@configclass
class BaseModelCfg:
    """
    Network Layer configs
    """

    @configclass
    class MLPConfig:
        input: int = MISSING
        output: int = MISSING
        shape: list[int] | None = None
        activation: str = MISSING
        dropout: float = 0.0
        batchnorm: bool = False

    @configclass
    class LSTMConfig:
        type: str = "LSTM"
        input_size: int = MISSING
        hidden_size: int = MISSING
        num_layers: int = MISSING
        dropout: float = MISSING
        batch_first: bool = True

    @configclass
    class GRUConfig:
        type: str = "GRU"
        input_size: int = MISSING
        hidden_size: int = MISSING
        num_layers: int = 1
        bias: bool = True
        batch_first: bool = True
        dropout: float = 0.0

    @configclass
    class CNNConfig:
        in_channels: int = MISSING
        out_channels: list[int] = MISSING
        kernel_size: list[tuple[int]] | tuple[int] = MISSING
        stride: list[int] | int = 1
        flatten: bool = True
        avg_pool: bool = False
        activation: str = MISSING
        batchnorm: bool | list[bool] = False
        max_pool: bool | list[bool] = False
        compress_MLP_layers: BaseModelCfg.MLPConfig | None = None

    @configclass
    class S4RNNConfig:
        input_size: int = MISSING
        hidden_size: int = MISSING
        num_layers: int = 1
        d_state: int = 32
        batch_first: bool = True
        dropout: float = 0.0
        prenorm: bool = False
        keep_states: bool = True

    @configclass
    class ResNetConfig:
        layers: list[int] = MISSING
        """Number of layers in each block"""
        layer_planes: list[int] = [64, 128, 256, 512]
        """Embedding dimension of different layers.

        Note: Original ResNet-18 uses [64, 128, 256, 512]"""
        layer_stride: list[int] = [2, 2, 2, 2]
        """Stride of the the convolution of each block"""
        groups: int = 1
        base_width: int = 64
        dilation: int = 1
        inplanes: int = 32
        """Number of input planes to the first block. Generated by a first convolution layer that works on the input image.
        The layer has a kernel size of 7."""
        input_channels: int = 1
        """Channel dimension of the input"""
        replace_stride_with_dilation: tuple[bool] = [False, False, False, False]
        """each element in the tuple indicates if we should replace the 2x2 stride with a dilated convolution instead"""
        downsample_MLP: BaseModelCfg.MLPConfig | None = None
        """MLP to downsample the the ResNet output"""
        avg_pool: bool = False
        """Whether to use average pooling to reduce the spational dimensions to 1x1"""
        individual_channel_encoding: bool = False
        """Process each channel individually through the network, share network weights across channels"""
        multi_scale_features: bool = True
        """Whether to extract features at different scales"""

    @configclass
    class PerceptNetCfg:
        layers: list[int] = [2, 2, 2, 2]
        """Number of layers in each block"""
        avg_pool: bool = False
        """Whether to use average pooling to reduce the spational dimensions to 1x1"""
        individual_channel_encoding: bool = True
        """Process each channel individually through the network, share network weights across channels"""
        img_size: tuple[int] = (180, 320)
        """Size of the input image"""

    max_grad_norm: float | None = 1.0
    """Maximum gradient norm for gradient clipping. If None, no clipping is performed"""
